FROM gemma3:12b 

# Instruction Tuning (ajuste fino con instrucciones)

SYSTEM """
Eres un experto lector y analizador de textos. Resumes y extraes informaciÃ³n clave de documentos largos.
Respondes de forma clara, concisa y precisa. Si no sabes la respuesta, di "Esa informaciÃ³n no aparece en el texto".
Utiliza un lenguaje formal y profesional. Evita jergas y coloquialismos.
No inventes informaciÃ³n. Basa tus respuestas Ãºnicamente en el contenido proporcionado.
Uiliza siempre el lenguaje espaÃ±ol de EspaÃ±a para tus respuestas.
"""

# Parameters (parÃ¡metros del modelo)

# MÃ¡s bajo para respuestas mÃ¡s deterministas y concisas. 0.0 â€“ 1.0 (algunos modelos aceptan hasta 2.0
PARAMETER temperature 0.5
# Similar a temperature, controla la aleatoriedad. 0.0 â€“ 1.0
PARAMETER top_p 0.3
# Controla la diversidad de las respuestas. 1 â€“ 1000 (por defecto muchos usan 40).       
PARAMETER top_k 40
# Evita que el modelo se repita. 0.0 â€“ 2.0 (tÃ­pico 1.0 â€“ 1.2)
PARAMETER repeat_penalty 0.6
# Limita la longitud de las respuestas para mantener la brevedad.
PARAMETER num_ctx 2000     

# ğŸ”¹ temperature (0.0 â€“ 1.0 * 2.0)

# Controla la aleatoriedad de la salida.

# 0.2 â†’ muy determinista: siempre da casi la misma respuesta.

# Ej: Preguntas â€œÂ¿quÃ© es Python?â€ â†’ siempre responde parecido.

# 1.0 â†’ balanceado.

# 1.5 â†’ creativo pero con riesgo de inventar cosas.

# ğŸ”¹ top_p (0.0 â€“ 1.0)

# Hace â€œnucleus samplingâ€: el modelo solo considera el subconjunto de palabras mÃ¡s probables hasta que suman p.

# 0.1 â†’ usa solo las palabras mÃ¡s seguras.

# Ej: Respuesta directa, sin adornos.

# 1.0 â†’ no filtra nada â†’ mÃ¡s variabilidad.

# ğŸ”¹ top_k (1 â€“ 1000)

# NÃºmero de palabras candidatas que se consideran en cada paso.

# top_k=10 â†’ elige entre 10 palabras mÃ¡s probables.

# Ej: respuestas mÃ¡s simples y repetitivas.

# top_k=200 â†’ mÃ¡s opciones, frases mÃ¡s variadas.

# ğŸ”¹ repeat_penalty (0.0 â€“ 2.0)

# Penaliza que el modelo repita tokens ya usados.

# 1.0 â†’ neutral.

# 1.2 â†’ evita repeticiones.

# Ej: en vez de repetir â€œmuy muy muy buenoâ€, lo cambia a â€œexcelente, estupendoâ€.

# <1 â†’ puede fomentar repeticiones (no recomendable).

# ğŸ”¹ num_ctx (mÃ¡x segÃºn modelo, p. ej. 8192 en gemma:27b)

# LÃ­mite de tokens en memoria (ventana de contexto).

# 256 â†’ solo tiene en cuenta lo Ãºltimo (respuestas cortas y baratas).

# 4096 â†’ recuerda mÃ¡s conversaciÃ³n/documentos.

# Si superas el lÃ­mite â†’ el modelo â€œolvidaâ€ lo primero.

OTROS PARAMETROS
# En Modelfile o durante ejecuciÃ³n
#PARAMETER temperature 0.7      # Creatividad (0.0-2.0)
#PARAMETER top_p 0.9           # Nucleus sampling (0.0-1.0)
#PARAMETER top_k 40            # Top-k sampling
#PARAMETER repeat_last_n 64    # Tokens para penalty de repeticiÃ³n
#PARAMETER repeat_penalty 1.1  # PenalizaciÃ³n por repetir
#PARAMETER num_ctx 2048        # Longitud de contexto
#PARAMETER num_batch 512       # TamaÃ±o de batch
#PARAMETER num_gpu 1           # NÃºmero de GPUs a usar
#PARAMETER main_gpu 0          # GPU principal
#PARAMETER low_vram false      # Modo bajo VRAM
#PARAMETER f16_kv true         # Usar float16 para cache KV
#PARAMETER logits_all false    # Calcular logits para todos tokens
#PARAMETER vocab_only false    # Solo cargar vocabulario
#PARAMETER use_mmap true       # Usar memory mapping
#PARAMETER use_mlock false     # Bloquear memoria
#PARAMETER num_thread 8        # Threads de CPU

